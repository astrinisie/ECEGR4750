{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Nets\n",
    "\n",
    "CNNs are designed to learn from spatial data - including images, 3d volumes, graphs, and more. For simplicity, we will focus on the application to image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's grab an example image, use a CNN layer to process it, and see if we can calculate the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_digits()\n",
    "image = data['data'][0,:] # images are packed as 1d vectors\n",
    "image = np.reshape(image, (8,8)) #reshape to be a 2d matrix\n",
    "print(image)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image data is represented on a per-pixel basis, so that each pixel is encoded with a brightness value. For color images, each pixel is encoded with an RGB value.\n",
    "\n",
    "Our CNN layer will slide a window across the image and produce a value at each step. The CNN has several parameters that can be specified to control how it scans.\n",
    "\n",
    "    Kernel size: how large the window is, in pixels.\n",
    "    Stride: how many pixels are skipped before each convolution is taken\n",
    "    Padding: whether extra pixels are added to the edges of images or not\n",
    "\n",
    "From these parameters, you can calculate the exact dimensions of the output image. Let's try some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare our data for our CNN. a torch CNN expects data to follow this format\n",
    "# [batch_size, channels, pixels horizontal, pixels vertical]\n",
    "image = np.reshape(image, (1, 1, 8, 8))\n",
    "image = torch.tensor(image, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels = 1, # our image is black and white, so only 1 channel (RGB has 3 channels)\n",
    "    out_channels = 1, # or number of kernels to train\n",
    "    kernel_size = (3, 3), # the window is of size 3x3\n",
    "    stride = (1, 1), # only move one pixel between convolutions\n",
    "    padding = (0, 0) # amount of extra pixels to add to the top and sides of our image\n",
    ")\n",
    "\n",
    "# pass the image through our CNN. what will be the shape of the output?\n",
    "output = conv(image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our image's input shape (8x8), or any image's shape, could we calculate the output from a given `Conv2d` layer ahead of time?\n",
    "\n",
    "Try writing the formula to do this by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_output_shape(image_size: list[int], kernel: list[int], stride: list[int], padding: list[int]):\n",
    "    # code here\n",
    "\n",
    "    return output_shape\n",
    "\n",
    "calculate_output_shape([8,8],[3,3],[1,1],[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each kernel (or filter) of our Conv2d is learned to extract features from the training data. CNNs come with a nice perk of being able to see what each kernel looks like after training. This can potentially tell us something about what our model has learned.\n",
    "\n",
    "We can demonstrate this with a simple example of a CNN trained to recognize the number 3. If we set the kernel size to equal the image size, we can see this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1kernel = nn.Conv2d(\n",
    "    in_channels = 1, # our image is black and white, so only 1 channel (RGB has 3 channels)\n",
    "    out_channels = 1, # or number of kernels to train\n",
    "    kernel_size = (8, 8), # the window is of size 3x3\n",
    "    stride = (1, 1), # only move one pixel between convolutions\n",
    "    padding = (0, 0) # amount of extra pixels to add to the top and sides of our image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some clever numpy indexing to grab all examples of 3s from our dataset\n",
    "x = data['data'][data['target'] == 3]\n",
    "y = data['target'][data['target'] == 3]\n",
    "\n",
    "# prep the data for training\n",
    "x = np.reshape(x, (len(x), 1, 8, 8))\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# lets set up a training loop, and save snapshots of each kernel every epoch\n",
    "optimizer = torch.optim.Adam(conv_1kernel.parameters(), lr = 0.001)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction = 'none')\n",
    "kernels = []\n",
    "\n",
    "kernels.append([param.detach().numpy().copy() for param in conv_1kernel.parameters()][0])\n",
    "for _ in range(10):\n",
    "    for i in range(len(x)):\n",
    "        optimizer.zero_grad()\n",
    "        yhat = torch.squeeze(conv_1kernel(x[i:i+1]))\n",
    "        loss = loss_fn(y[i], yhat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    kernels.append([param.detach().numpy().copy() for param in conv_1kernel.parameters()][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 11, figsize=(30,3))\n",
    "\n",
    "for i in range(len(kernels)):\n",
    "    axs[i].imshow(kernels[i][0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, our kernel is learning to look more and more like a 3. A typical CNN will have hundreds to thousands of kernels that are far smaller than the size of the input image, so interpreting them visually can be more challenging.\n",
    "\n",
    "Also, CNNs can easily \"stack\" such that the output of one `Conv2d` layer is fed into the input of the next. Here, I have the first layer of a CNN written out. Fill in the following layers that are commented out, so that the final output is of a shape `(*,64,1,1)`. Then, add a final fully connected layer that converts the output into shape `(*,10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img = torch.rand((1,1,28,28))\n",
    "\n",
    "conv1 = nn.Conv2d(\n",
    "    in_channels = 1, \n",
    "    out_channels = 16,\n",
    "    kernel_size = (5, 5), \n",
    "    stride = (1, 1),\n",
    "    padding = (0, 0),\n",
    ")\n",
    "# maxpool1 = nn.MaxPool2d(kernel_size= \n",
    "#                        stride = \n",
    "#                        padding= \n",
    "# )\n",
    "# conv2 = nn.Conv2d(\n",
    "#     in_channels = \n",
    "#     out_channels = \n",
    "#     kernel_size = \n",
    "#     stride = \n",
    "#     padding = \n",
    "# )\n",
    "# maxpool2 = nn.MaxPool2d(kernel_size=\n",
    "#                        stride = \n",
    "#                        padding= \n",
    "# )\n",
    "# linear1 = nn.Linear(in_features=\n",
    "#                     out_features= \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the final shape, think about how you have transformed the data from a spatially-sensitive 2d matrix, to a kernel-sensitive 1d vector. Essentially, you have compressed the image. Now create a class from your layers so we can re-use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's grab a real image dataset from `torch`. We can use Fashion MNIST, which is like the MNIST digits dataset but more challenging to learn. Check it out here https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "`pip install torchvision` if you don't have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# creates a data object\n",
    "fashion_mnist_train = torchvision.datasets.FashionMNIST(root = './', download=True, train=True)\n",
    "fashion_mnist_val = torchvision.datasets.FashionMNIST(root = './', download=True, train=False)\n",
    "\n",
    "# we can fetch an item from the data like so, which gives us the image in PIL form, and the class label as an int\n",
    "print(fashion_mnist_train.__getitem__(0))\n",
    "\n",
    "# we can convert the image to a numpy array like so\n",
    "img = fashion_mnist_train.__getitem__(0)[0]\n",
    "img = np.array(img)\n",
    "\n",
    "# and here is a shoe\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed you along, I have code here that processes the data into a format ready-to-ingest by our `CustomDataloader`. Because 60k images is a lot to hold in memory, we will just train on the first 1000. You can run `del fashion_mnist_train` and likewise for val if your notebook starts crashing to free up some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a training loop to train your new model on the image data. Note that when I did this, I found model initialization and hyperparameters made a very large difference in whether the model converged or not. You may need to try the same parameters multiple times to get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
