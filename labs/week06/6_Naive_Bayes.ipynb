{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "![alt text](bayes.png \"Bayes being naive - there's a calculator right there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write up the code to do *gaussian* naive bayes from scratch, and see if we can visualize the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "x, y = make_blobs(n_samples=300, cluster_std=4, centers=2, random_state=42)\n",
    "\n",
    "# We can use some clever numpy indexing to fetch the indices in the array that correspond to a certain value\n",
    "class0 = x[y==0]\n",
    "class1 = x[y==1]\n",
    "\n",
    "# Take a look at the data\n",
    "print('x shape:',x.shape)\n",
    "print('y shape:',y.shape)\n",
    "\n",
    "plt.title('Data by class')\n",
    "plt.scatter(class0[:,0],class0[:,1])\n",
    "plt.scatter(class1[:,0],class1[:,1])\n",
    "plt.legend(['Class 0','Class 1']);\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe some overlap in the classes. A naive bayes assumes (usually wrongly) that our features are independent of each other. Let's try to describe the probability distribution we think our data is coming from. Thankfully, they already look like gaussian blobs!\n",
    "\n",
    "This is the gaussian function. It is probably the most utilized function in all of machine learning, because it has so many nice statistical properties.\n",
    "\n",
    "$$\n",
    "p(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "$\\mu$ is the mean and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before proceeding, let's define what an actual multivariate gaussian function is in code\n",
    "\n",
    "def multivariate_gaussian(mu: np.ndarray, sigma: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
    "    _, num_features = x.shape\n",
    "    covariance = np.diag(sigma ** 2)\n",
    "    # compute constants\n",
    "    constant = 1 / ((2 * np.pi) ** (num_features / 2) * np.linalg.det(covariance) ** 0.5)\n",
    "    # compute the exponent\n",
    "    x_shift = x - mu\n",
    "    exp_term = -0.5 * np.sum(np.dot(x_shift, np.linalg.inv(covariance)) * x_shift, axis=1)\n",
    "    # compute the probability density\n",
    "    pdf_values = constant * np.exp(exp_term)\n",
    "    return pdf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the mean and std of the features that correspond to each class?\n",
    "# note 'axis=0' indicates what dimension we are taking the mean over\n",
    "print(f\"Class 0 feature means {np.mean(class0, axis=0)} \\n and standard deviations {np.std(class0, axis=0)}\") \n",
    "print(f\"Class 1 feature means {np.mean(class1, axis=0)} \\n and standard deviations {np.std(class1, axis=0)}\") \n",
    "\n",
    "# We can then drop a gaussian for each class, using our means and stds\n",
    "# A contour plot can visualize it for us nicely\n",
    "# First, create a grid of points\n",
    "f1 = np.linspace(-20, 20, 100)\n",
    "f2 = np.linspace(-20, 20, 100)\n",
    "f1,f2 = np.meshgrid(f1, f2)\n",
    "grid_pts = np.vstack(np.dstack((f1, f2)))\n",
    "\n",
    "# Evaluate every grid point using our multivariate guassian function\n",
    "# for class 0\n",
    "c0_mu = np.mean(class0, axis=0)\n",
    "c0_std = np.std(class0, axis=0)\n",
    "grid_vals_c0 = multivariate_gaussian(c0_mu, c0_std, grid_pts)\n",
    "grid_vals_c0 = np.reshape(grid_vals_c0, (100,100))\n",
    "\n",
    "# for class 1\n",
    "c1_mu = np.mean(class1, axis=0)\n",
    "c1_std = np.std(class1, axis=0)\n",
    "grid_vals_c1 = multivariate_gaussian(c1_mu, c1_std, grid_pts)\n",
    "grid_vals_c1 = np.reshape(grid_vals_c1, (100,100))\n",
    "\n",
    "# plot it out\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Class-centered gaussians')\n",
    "plt.contour(f1, f2, grid_vals_c0, levels = 100, colors='blue', alpha=0.3)\n",
    "plt.contour(f1, f2, grid_vals_c1, levels = 100, colors='orange', alpha=0.3)\n",
    "plt.scatter(class0[:,0],class0[:,1])\n",
    "plt.scatter(class1[:,0],class1[:,1])\n",
    "plt.xlabel('Feature 1');\n",
    "plt.ylabel('Feature 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above gaussians, write your own code to evaluate the dataset and calculate the overall accuracy of this naive bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple predict function that iterates through our data, calculates probabilities, and goes with the higher number\n",
    "yhat = []\n",
    "for sample in x:\n",
    "    sample = np.expand_dims(sample,0)\n",
    "    #use the multivariate gaussian function to determine the probability densities for each class\n",
    "    #predict the class label accordingly\n",
    "    continue\n",
    "\n",
    "#then calculate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaussians we have defined can be used to also *generate* data. Rather than do gradient descent to fit features to labels, we have instead attempted to describe the underlying \"truth\" of how the data was made in the first place.\n",
    "\n",
    "Note that **probability density** is not the same as **probability**. We use densities here because we have continuous values. If we wanted to convert them into probabilities, we would need to integrate the density over some range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using the above gaussians to generate brand new data. Make a scatter plot and see if it looks like your original data\n",
    "# https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html <- use this!\n",
    "\n",
    "rand0 = np.random.normal(c0_mu, c0_std, (100,2))\n",
    "rand1 = np.random.normal(c1_mu, c1_std, (100,2))\n",
    "\n",
    "plt.scatter(rand0[:,0],rand0[:,1])\n",
    "plt.scatter(rand1[:,0],rand1[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes\n",
    "\n",
    "Before, we described our features using a gaussian. However, if our features are binary in nature, then a gaussian would be a poor choice. Instead, we should use a binary distribution, like the Bernoulli distribution. This is a special case of a more general multinomial distribution.\n",
    "\n",
    "Load in the thrombospondin molecular data, which consists of binary fingerprint data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = np.load('../homeworks/thrombo_fingerprints.npy')\n",
    "potencies = np.load('../homeworks/thrombo_potencies.npy')\n",
    "#set a potency threshold of 200, make it binary\n",
    "y = 1. * np.array(potencies < 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare how the choice of distribution (gaussian vs bernoulli) impacts the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "\n",
    "bernoulli_clf = BernoulliNB()\n",
    "bernoulli_clf.fit(fps, y)\n",
    "yhat = bernoulli_clf.predict(fps)\n",
    "print('Bernoulli classifier accuracy:', np.sum(yhat == y) / len(y))\n",
    "\n",
    "gaussian_clf = GaussianNB()\n",
    "gaussian_clf.fit(fps, y)\n",
    "yhat = gaussian_clf.predict(fps)\n",
    "print('Gaussian classifier accuracy:', np.sum(yhat == y) / len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize how each bit in the fingerprint has a certain probability of indicating the molecule belongs to the potent or not potent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,4))\n",
    "plt.title('Potent molecules')\n",
    "plt.bar(range(2048),np.exp(bernoulli_clf.feature_log_prob_[0]), color = 'red')\n",
    "plt.xlabel('Fingerprint bit index')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.figure(1, figsize=(16,4))\n",
    "plt.title('Inactive molecules')\n",
    "plt.bar(range(2048),np.exp(bernoulli_clf.feature_log_prob_[1]), color = 'black')\n",
    "plt.xlabel('Fingerprint bit index')\n",
    "plt.ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it that the sum of probabilities for some of the bits seems to exceed 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
