{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Part 2 - Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "So far, we have also dealt with pre-processed datasets. Preparing data for machine learning is usualy more time consuming than actually developing and training models, so let's start with a little rougher dataset. Load in the wages dataset that describes characteristics of a group of individuals. We will build a model that takes in demographic info and predicts whether an individual has health insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/wages.csv')\n",
    "print(len(df))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its apparent we have a mix of categorical and numeric data. Each type of data requires a different method to prepare it.\n",
    "\n",
    "- Categorical: use *one hot encoding*\n",
    "- Numeric: if it is logarithmically distributed, take the log of the data, then standardize to a mean of 0 and standard deviation of 1\n",
    "\n",
    "However, before processing the data, we have to ask ourselves an important question.\n",
    "\n",
    "Do we, as humans, expect all these features to be relevant for predicting health insurance? What are the ethical considerations of including this data?\n",
    "\n",
    "Because we don't want to introduce a racial bias to our model, we will drop race as an input feature. The other features seem potentially relevant for our task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of data in each column\n",
    "df = df.drop('race', axis=1)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric data\n",
    "\n",
    "We will examine the distribution of each feature, and determine the best way to transform it into a \"numerically stable\" region, aka centered about 0 and scaled roughly with a standard deviation of 1. This ensures that features are treated similarly by the network, and will promote numerical stability during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use that information to visualize the distribution of data for each numeric column\n",
    "numeric_columns = [\"year\",\"age\",\"wage\"]\n",
    "fig, axs = plt.subplots(3,1, figsize=(10,10))\n",
    "for i in range(3):\n",
    "    axs[i].hist(df[numeric_columns[i]], bins = 25)\n",
    "    axs[i].set_xlabel(numeric_columns[i], fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function that standardizes numeric data that we can apply to our dataframe\n",
    "def standardize_numeric(series: pd.Series, use_log: bool = False) -> pd.Series:\n",
    "    # write code here that optionally takes the log of in the input series, then standardizes it\n",
    "\n",
    "df['year_st'] = standardize_numeric(df['year'], False)\n",
    "df['age_st'] = standardize_numeric(df['age'], False)\n",
    "df['wage_st'] = standardize_numeric(df['wage'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize transformed numeric data\n",
    "numeric_columns = [\"year_st\",\"age_st\",\"wage_st\"]\n",
    "fig, axs = plt.subplots(3,1, figsize=(10,10))\n",
    "for i in range(3):\n",
    "    axs[i].hist(df[numeric_columns[i]], bins = 25)\n",
    "    axs[i].set_xlabel(numeric_columns[i], fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data\n",
    "\n",
    "We will identify the potential classes for each column, and then one-hot encode them. One-hot encoding entails representing classes as an integer encoding. For instance, given 3 possible labels of [ `Healthy`, `Sick`, `Unknown` ], say we have a data sample where an individual is labeled as 'Sick'. We can encode it like this\n",
    "\n",
    "        [0, 1, 0]\n",
    "\n",
    "Where each value represents a class. '1' means the class is labelled as true for that entry. Technically, you can have multiple 1s in a single encoding, thus meaning it is a *multi-label* task, or that your classes are not mutually exclusive. In the scenario above, our classes are mutually exclusive, thus we can expect only a single 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of data in each column\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all labels for each column, and the number of appearances of each\n",
    "categoric_columns = ['sex','maritl','education','region','jobclass','health','health_ins']\n",
    "for i in range(len(categoric_columns)):\n",
    "    print(f\"\\nColumn: {categoric_columns[i]}\")\n",
    "    counts = df[categoric_columns[i]].value_counts()\n",
    "    for label, count in counts.items():\n",
    "        print(f\"Label: '{label}' | Frequency: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we keep all these columns? Why or why not?\n",
    "\n",
    "Are any of these features more **ordinal** in nature? As in, you could place each label on an axis of some kind and assign a value to it?\n",
    "\n",
    "Are any of our labels particularly imbalanced? Why is that potentially a bad thing and what would you do to fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any columns you decide are not good to keep\n",
    "# use df.drop to drop any columns you don't want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one-hot encode the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code efficiently converts labels in columns into columns themselves, then populates it with 1s and 0s accordingly\n",
    "# keep_categoric_columns = # populate here\n",
    "for col in keep_categoric_columns:\n",
    "    df = df.join(pd.get_dummies(df[col], dtype = 'int'), how = 'outer')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion\n",
    "\n",
    "The dataframe is processed, and now must simply be converted into a tensor for our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list out all your finalized INPUT and OUTPUT feature columns.\n",
    "# features = # populate here\n",
    "\n",
    "# write down your TARGET columns\n",
    "# Because our task is a binary classification task (has insurance, or doesn't), we can actually just represent the target \n",
    "# as a single vector of 0s and 1s, where 0 indicates no insurance, and 1 indicates possession of insurance.\n",
    "# target = # populate here\n",
    "\n",
    "# create new dataframes of your inputs and outputs\n",
    "train_df = df[features + target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will split our data. However, due to the imbalance of our target class, we will do *stratified* splitting. That means we won't accidentally over-represent a certain class in one our splits due to random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split train and val\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_df[features], train_df[target], train_size=0.6, stratify=train_df[target])\n",
    "\n",
    "# split again to get a test set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, train_size=0.5, stratify=y_val)\n",
    "\n",
    "print(\"x train: \",x_train.shape, \"y train:\", y_train.shape)\n",
    "print(\"x val: \",x_val.shape, \"y train:\", y_val.shape)\n",
    "print(\"x test: \",x_test.shape, \"y test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's convert our data into tensors, and save it to disk so we don't have to do this again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes -> numpy arrays -> tensors\n",
    "# write conversion code here\n",
    "\n",
    "# store it in a dict that we can save out as a single file\n",
    "data_dict = {'x_train':x_train, 'x_val':x_val, 'x_test':x_test, 'y_train':y_train, 'y_val':y_val, 'y_test':y_test}\n",
    "\n",
    "# save it to local data directory\n",
    "torch.save(data_dict, 'data/wages_processed.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and confirm it works by loading it back in as a tensor\n",
    "data_dict = torch.load('data/wages_processed.pt')\n",
    "print(data_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have completed data preparation! It can be a tedious procedure at times, but probably 80% of the time, the source of errors during training come from data preparation, so it is absolutely necessary to know exactly how your data was gathered out in the wild, how it was filtered and prepared, and how it was transformed for your ML model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
